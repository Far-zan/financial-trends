{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd88e827-9239-41ef-a01f-9f3a57f00e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb476436-c096-4040-9960-8cae345094e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyalex sentence-transformers pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7d307c-5868-44b9-bb73-435f93e055eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# 1. Imports and configuration\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from pyalex import Topics, Works, config as pyalex_config\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pyalex_config.email = \"farzan.saif@gmail.com\"  # TODO: change this\n",
    "\n",
    "DATA_DIR = Path(\"/dbfs/FileStore/fincrime/data\")\n",
    "EXPORT_DIR = Path(\"/dbfs/FileStore/fincrime/export_for_powerbi\")\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR, EXPORT_DIR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a2b138f-cd85-4027-8989-0cf2fbd73837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# 2. Load OpenAlex TOPICS via PyAlex (once)\n",
    "topics_records = list(chain.from_iterable(Topics().paginate(per_page=200)))\n",
    "\n",
    "len(topics_records), topics_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247fbf78-6f54-4725-8025-4273b400b22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# 3. Put topics into a pandas DataFrame and build text_for_embedding\n",
    "\n",
    "topics_df = pd.DataFrame(topics_records)\n",
    "topics_df.head()\n",
    "\n",
    "topics_df[\"keywords_str\"] = topics_df[\"keywords\"].apply(\n",
    "    lambda x: \" \".join(x) if isinstance(x, list) else (x or \"\")\n",
    ")\n",
    "\n",
    "topics_df[\"text_for_embedding\"] = (\n",
    "    topics_df[\"display_name\"] + \" | \" +\n",
    "    topics_df[\"description\"] + \" | \" +\n",
    "    topics_df[\"keywords_str\"]\n",
    ")\n",
    "\n",
    "topics_df[[\"id\", \"display_name\", \"text_for_embedding\"]].head()\n",
    "\n",
    "topics_path = DATA_DIR / \"openalex_topics.csv\"\n",
    "topics_df.to_csv(topics_path, index=False)\n",
    "topics_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5203f484-0b44-48e6-913f-7016d6719344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# 4. Compute topic embeddings with SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "topic_texts = topics_df[\"text_for_embedding\"].tolist()\n",
    "\n",
    "topic_embeddings = model.encode(\n",
    "    topic_texts,\n",
    "    batch_size=128,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "\n",
    "len(topic_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ceaf7c-4c5e-49a0-9da3-69ec1a6b92a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# 5. Helper: select topics for a given free-text query\n",
    "\n",
    "def select_topics_for_query(\n",
    "    query: str,\n",
    "    topics_df: pd.DataFrame,\n",
    "    topic_embeddings,\n",
    "    top_k: int = 5,\n",
    "    min_similarity: float = 0.4,\n",
    "    dropoff_ratio: float = 0.75,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a text query, return a small DataFrame of the most relevant OpenAlex topics.\n",
    "    \"\"\"\n",
    "    query_emb = model.encode(query, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(query_emb, topic_embeddings)[0]\n",
    "\n",
    "    top_scores, top_idx = scores.topk(k=top_k)\n",
    "    top_scores = top_scores.cpu().tolist()\n",
    "    top_idx = top_idx.cpu().tolist()\n",
    "\n",
    "    selected = []\n",
    "    best = top_scores[0]\n",
    "\n",
    "    for score, idx in zip(top_scores, top_idx):\n",
    "        if score < min_similarity:\n",
    "            continue\n",
    "        if score < best * dropoff_ratio:\n",
    "            continue\n",
    "\n",
    "        row = topics_df.iloc[idx].copy()\n",
    "        row[\"similarity\"] = float(score)\n",
    "        selected.append(row)\n",
    "\n",
    "    if not selected:\n",
    "        row = topics_df.iloc[top_idx[0]].copy()\n",
    "        row[\"similarity\"] = float(top_scores[0])\n",
    "        selected.append(row)\n",
    "\n",
    "    return pd.DataFrame(selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af4ff41-8087-4fb0-b195-0a7dfc8b3a1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# 6. Choose a user query and inspect selected topics\n",
    "\n",
    "user_query = \"crypto money laundering\"  # TODO: adjust for different runs\n",
    "\n",
    "selected_topics_df = select_topics_for_query(\n",
    "    query=user_query,\n",
    "    topics_df=topics_df,\n",
    "    topic_embeddings=topic_embeddings,\n",
    "    top_k=5,\n",
    "    min_similarity=0.4,\n",
    "    dropoff_ratio=0.75,\n",
    ")\n",
    "\n",
    "print(\"User query:\", user_query)\n",
    "selected_topics_df[[\"id\", \"display_name\", \"similarity\"]]\n",
    "\n",
    "\n",
    "selected_topics_df[\"topic_short_id\"] = selected_topics_df[\"id\"].str.replace(\n",
    "    \"https://openalex.org/\", \"\",\n",
    "    regex=False\n",
    ")\n",
    "\n",
    "selected_topics_df[[\"id\", \"topic_short_id\", \"display_name\", \"similarity\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f77ab1d-f47c-409e-8902-072111a6a7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# 8. Fetch works for selected topics via PyAlex and save to JSONL\n",
    "\n",
    "raw_jsonl_path = DATA_DIR / \"works_raw.jsonl\"\n",
    "\n",
    "if raw_jsonl_path.exists():\n",
    "    raw_jsonl_path.unlink()\n",
    "\n",
    "from_year = 2020\n",
    "to_year = 2024\n",
    "per_page = 200\n",
    "max_pages_per_topic = 50  \n",
    "\n",
    "total = 0\n",
    "with raw_jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in selected_topics_df.iterrows():\n",
    "        topic_short_id = row[\"topic_short_id\"]\n",
    "        topic_name = row[\"display_name\"]\n",
    "\n",
    "        print(f\"\\n=== Fetching works for topic {topic_short_id} – {topic_name} ===\")\n",
    "\n",
    "        from_date = f\"{from_year}-01-01\"\n",
    "        to_date = f\"{to_year}-12-31\"\n",
    "\n",
    "        pager = (\n",
    "            Works()\n",
    "            .filter(\n",
    "                topics={\"id\": [topic_short_id]},\n",
    "                from_publication_date=from_date,\n",
    "                to_publication_date=to_date,\n",
    "            )\n",
    "            .paginate(per_page=per_page)\n",
    "        )\n",
    "\n",
    "        page_count = 0\n",
    "        topic_record_count = 0\n",
    "\n",
    "        for page in pager:\n",
    "            page_count += 1\n",
    "            print(f\"  → Page {page_count} ... \", end=\"\")\n",
    "\n",
    "            page_len = 0\n",
    "            for rec in page:\n",
    "                rec[\"source_topic_id\"] = topic_short_id\n",
    "                f.write(json.dumps(rec) + \"\\n\")\n",
    "                total += 1\n",
    "                topic_record_count += 1\n",
    "                page_len += 1\n",
    "\n",
    "            print(f\"{page_len} works\")\n",
    "\n",
    "            if page_count >= max_pages_per_topic:\n",
    "                print(f\"  (Reached max_pages_per_topic = {max_pages_per_topic}, stopping early)\")\n",
    "                break\n",
    "\n",
    "        print(f\"Completed topic {topic_short_id}: {topic_record_count} works written.\")\n",
    "\n",
    "print(f\"\\nTotal works written across all topics: {total}\")\n",
    "print(f\"Saved to {raw_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aad0dfc-05bd-4eb5-b606-3a9986163a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# 9. Load raw works into pandas\n",
    "\n",
    "raw_df = pd.read_json(raw_jsonl_path, lines=True)\n",
    "raw_df.shape\n",
    "\n",
    "raw_df[[\"id\", \"title\", \"publication_year\", \"cited_by_count\", \"source_topic_id\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6800e6-9dc3-4580-9e22-fc53d76d1bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def decode_abstract(abstract_index):\n",
    "    if not isinstance(abstract_index, dict):\n",
    "        return \"\"\n",
    "    pos_word = [(pos, word) for word, positions in abstract_index.items() for pos in positions]\n",
    "    return \" \".join(word for _, word in sorted(pos_word))\n",
    "\n",
    "# Decode abstract from inverted index\n",
    "raw_df[\"abstract\"] = raw_df[\"abstract_inverted_index\"].apply(decode_abstract)\n",
    "\n",
    "# Simple flat works table: core metadata\n",
    "works_cols = [\n",
    "    \"id\",\n",
    "    \"doi\",\n",
    "    \"title\",\n",
    "    \"abstract\",  \n",
    "    \"publication_year\",\n",
    "    \"publication_date\",\n",
    "    \"cited_by_count\",\n",
    "    \"type\",\n",
    "    \"source_topic_id\",\n",
    "]\n",
    "\n",
    "works_flat_df = raw_df[works_cols].drop_duplicates(\"id\")\n",
    "works_flat_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "821196ee-dfd3-4d90-b5ea-a9d6b340cd68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# 11. Explode authorships and institutions\n",
    "\n",
    "# 11.1 Explode authorships\n",
    "auth_df = raw_df[[\"id\", \"authorships\"]].explode(\"authorships\")\n",
    "auth_df = auth_df.dropna(subset=[\"authorships\"])\n",
    "\n",
    "auth_df[\"author_id\"] = auth_df[\"authorships\"].apply(\n",
    "    lambda a: a.get(\"author\", {}).get(\"id\") if isinstance(a, dict) else None\n",
    ")\n",
    "auth_df[\"author_name\"] = auth_df[\"authorships\"].apply(\n",
    "    lambda a: a.get(\"author\", {}).get(\"display_name\") if isinstance(a, dict) else None\n",
    ")\n",
    "auth_df[\"author_position\"] = auth_df[\"authorships\"].apply(\n",
    "    lambda a: a.get(\"author_position\") if isinstance(a, dict) else None\n",
    ")\n",
    "\n",
    "# 11.2 Explode institutions within each authorship\n",
    "auth_df[\"institutions\"] = auth_df[\"authorships\"].apply(\n",
    "    lambda a: a.get(\"institutions\", []) if isinstance(a, dict) else []\n",
    ")\n",
    "inst_df = auth_df.explode(\"institutions\")\n",
    "inst_df = inst_df.dropna(subset=[\"institutions\"])\n",
    "\n",
    "inst_df[\"institution_id\"] = inst_df[\"institutions\"].apply(\n",
    "    lambda i: i.get(\"id\") if isinstance(i, dict) else None\n",
    ")\n",
    "inst_df[\"institution_name\"] = inst_df[\"institutions\"].apply(\n",
    "    lambda i: i.get(\"display_name\") if isinstance(i, dict) else None\n",
    ")\n",
    "inst_df[\"institution_country_code\"] = inst_df[\"institutions\"].apply(\n",
    "    lambda i: i.get(\"country_code\") if isinstance(i, dict) else None\n",
    ")\n",
    "\n",
    "inst_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b662465-8ccf-42da-94a9-ad38a9986c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "# 12. Aggregations for analysis / Power BI\n",
    "\n",
    "# 12.1 Publications per year\n",
    "pubs_per_year_df = (\n",
    "    works_flat_df\n",
    "    .dropna(subset=[\"publication_year\"])\n",
    "    .groupby(\"publication_year\")\n",
    "    .agg(publication_count=(\"id\", \"nunique\"))\n",
    "    .reset_index()\n",
    "    .sort_values(\"publication_year\")\n",
    ")\n",
    "\n",
    "pubs_per_year_df.head()\n",
    "\n",
    "# 12.2 Publications by country\n",
    "inst_with_year_df = inst_df.merge(\n",
    "    works_flat_df[[\"id\", \"publication_year\"]],\n",
    "    on=\"id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "pubs_by_country_df = (\n",
    "    inst_with_year_df\n",
    "    .dropna(subset=[\"institution_country_code\"])\n",
    "    .groupby(\"institution_country_code\")\n",
    "    .agg(publication_count=(\"id\", \"nunique\"))\n",
    "    .reset_index()\n",
    "    .sort_values(\"publication_count\", ascending=False)\n",
    ")\n",
    "\n",
    "pubs_by_country_df.head()\n",
    "\n",
    "# 12.3 Publications by institution\n",
    "pubs_by_institution_df = (\n",
    "    inst_df\n",
    "    .dropna(subset=[\"institution_id\"])\n",
    "    .groupby([\"institution_id\", \"institution_name\"])\n",
    "    .agg(publication_count=(\"id\", \"nunique\"))\n",
    "    .reset_index()\n",
    "    .sort_values(\"publication_count\", ascending=False)\n",
    ")\n",
    "\n",
    "pubs_by_institution_df.head()\n",
    "\n",
    "# 12.4 Top cited works\n",
    "top_cited_df = (\n",
    "    works_flat_df\n",
    "    .dropna(subset=[\"cited_by_count\"])\n",
    "    .sort_values(\"cited_by_count\", ascending=False)\n",
    "    .head(100)\n",
    ")\n",
    "\n",
    "top_cited_df[[\"title\", \"publication_year\", \"cited_by_count\"]].head()\n",
    "\n",
    "# COMMAND ----------\n",
    "# 13. Export CSVs for Power BI (stored in DBFS)\n",
    "\n",
    "pubs_per_year_df.to_csv(EXPORT_DIR / \"publications_per_year.csv\", index=False)\n",
    "pubs_by_country_df.to_csv(EXPORT_DIR / \"top_countries.csv\", index=False)\n",
    "pubs_by_institution_df.to_csv(EXPORT_DIR / \"top_institutions.csv\", index=False)\n",
    "top_cited_df.to_csv(EXPORT_DIR / \"top_cited.csv\", index=False)\n",
    "works_flat_df.to_csv(EXPORT_DIR / \"works_tidy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe82a8f-f33f-41bf-ae13-453c26482ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === SECTOR MAPPING ===\n",
    "#sourced from pg 7 https://www.austrac.gov.au/sites/default/files/2024-07/2024%20AUSTRAC%20Money%20Laundering%20NRA.pdf\n",
    "## condensed and individual keywords formed using an LLM\n",
    "\n",
    "SECTOR_KEYWORDS = {\n",
    "    \"banking\": [\n",
    "        \"major banks\", \"domestic banks\", \"foreign bank branches\", \"mutual banks\",\n",
    "        \"non-bank lenders\", \"financial institutions\", \"stockbroker\", \"wealth management\",\n",
    "        \"securities dealer\", \"custodian\", \"superannuation\", \"managed investment scheme\"\n",
    "    ],\n",
    "    \"remittance\": [\n",
    "        \"remittance\", \"money transfer\", \"registered remittance\", \"unregistered remittance\"\n",
    "    ],\n",
    "    \"digital currency\": [\n",
    "        \"cryptocurrency\", \"bitcoin\", \"ethereum\", \"digital asset\",\n",
    "        \"digital currency exchange\", \"DCE\", \"digital currency\", \"crypto\"\n",
    "    ],\n",
    "    \"gambling\": [\n",
    "        \"gambling\", \"casino\", \"betting agency\", \"corporate bookmaker\",\n",
    "        \"pubs and clubs\", \"on-course bookmaker\", \"offshore gambling\", \"junket tour\"\n",
    "    ],\n",
    "    \"real estate\": [\n",
    "        \"real estate\", \"property\", \"real estate agent\"\n",
    "    ],\n",
    "    \"bullion\": [\n",
    "        \"bullion\", \"precious metals\", \"gold\", \"bullion dealer\"\n",
    "    ],\n",
    "    \"professional services\": [\n",
    "        \"accountant\", \"lawyer\", \"legal services\", \"legal structure\",\n",
    "        \"trust\", \"company service provider\", \"offshore service provider\"\n",
    "    ],\n",
    "    \"cash-intensive business\": [\n",
    "        \"cash\", \"cash smuggling\", \"declared cash\", \"undeclared cash\",\n",
    "        \"cash-intensive\", \"transfer of value\", \"store of value\"\n",
    "    ],\n",
    "    \"luxury goods\": [\n",
    "        \"luxury goods\", \"luxury vehicle\", \"luxury watercraft\"\n",
    "    ],\n",
    "    \"customs and logistics\": [\n",
    "        \"customs broker\", \"freight\", \"shipping\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def classify_sectors(df: pd.DataFrame, text_cols=[\"title\", \"abstract\"]):\n",
    "    def extract_sectors(text):\n",
    "        matches = set()\n",
    "        for sector, keywords in SECTOR_KEYWORDS.items():\n",
    "            for kw in keywords:\n",
    "                if kw.lower() in text.lower():\n",
    "                    matches.add(sector)\n",
    "        return list(matches)\n",
    "\n",
    "    df[\"combined_text\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    df[\"sectors\"] = df[\"combined_text\"].apply(extract_sectors)\n",
    "    return df.drop(columns=[\"combined_text\"])\n",
    "\n",
    "# === HIGH-RISK AND MONITORED COUNTRIES EXTRACTION ===\n",
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "#https://www.fatf-gafi.org/content/fatf-gafi/en/publications/High-risk-and-other-monitored-jurisdictions/Call-for-action-june-2025.html\n",
    "# Source: FATF \"Call for Action\" (June 2025)\n",
    "# https://www.fatf-gafi.org/content/fatf-gafi/en/publications/High-risk-and-other-monitored-jurisdictions/Call-for-action-june-2025.html\n",
    "HIGH_RISK_COUNTRIES = [\n",
    "    \"North Korea\", \"Iran\", \"Myanmar\"\n",
    "]\n",
    "\n",
    "# Source: FATF \"Jurisdictions under Increased Monitoring\" (June 2025)\n",
    "# https://www.fatf-gafi.org/en/publications/High-risk-and-other-monitored-jurisdictions/increased-monitoring-june-2025.html\n",
    "FATF_MONITORED_COUNTRIES = [\n",
    "    \"Algeria\", \"Angola\", \"Bolivia\", \"Bulgaria\", \"Burkina Faso\", \"Cameroon\",\n",
    "    \"Côte d'Ivoire\", \"Democratic Republic of the Congo\", \"Haiti\", \"Kenya\",\n",
    "    \"Lao PDR\", \"Lebanon\", \"Monaco\", \"Mozambique\", \"Namibia\", \"Nepal\", \"Nigeria\",\n",
    "    \"South Africa\", \"South Sudan\", \"Syria\", \"Venezuela\", \"Vietnam\",\n",
    "    \"Virgin Islands (UK)\", \"Yemen\"\n",
    "]\n",
    "\n",
    "# === REGIONAL & STRATEGIC PARTNERS ===\n",
    "# Source: AUSTRAC strategic focus regions and collaboration\n",
    "\n",
    "REGIONAL_GROUPS = {\n",
    "    \"Pacific\": [\"Fiji\", \"Samoa\", \"Vanuatu\", \"Tonga\", \"Papua New Guinea\"],\n",
    "    \"Southeast Asia\": [\"Thailand\", \"Malaysia\", \"Vietnam\", \"Indonesia\", \"Cambodia\", \"Philippines\"],\n",
    "    \"AUSTRAC Partners (Five Eyes)\": [\"Australia\", \"New Zealand\", \"United Kingdom\", \"United States\", \"Canada\"]\n",
    "}\n",
    "\n",
    "def extract_jurisdictions(text):\n",
    "    labels = set()\n",
    "\n",
    "    for country in HIGH_RISK_COUNTRIES:\n",
    "        if re.search(rf\"\\b{re.escape(country)}\\b\", text, re.IGNORECASE):\n",
    "            labels.add(\"High-Risk: \" + country)\n",
    "\n",
    "    for country in FATF_MONITORED_COUNTRIES:\n",
    "        if re.search(rf\"\\b{re.escape(country)}\\b\", text, re.IGNORECASE):\n",
    "            labels.add(\"FATF Monitored: \" + country)\n",
    "\n",
    "    for group, members in REGIONAL_GROUPS.items():\n",
    "        if any(re.search(rf\"\\b{re.escape(c)}\\b\", text, re.IGNORECASE) for c in members):\n",
    "            labels.add(f\"Region: {group}\")\n",
    "\n",
    "    return list(labels)\n",
    "\n",
    "def tag_jurisdictions(df: pd.DataFrame, text_cols=[\"title\", \"abstract\"]):\n",
    "    df[\"combined_text\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    df[\"jurisdictions\"] = df[\"combined_text\"].apply(extract_jurisdictions)\n",
    "    return df.drop(columns=[\"combined_text\"])\n",
    "\n",
    "\n",
    "# === PREDICATE CRIME CLASSIFICATION ===\n",
    "#sourced from pg 6 https://www.austrac.gov.au/sites/default/files/2024-07/2024%20AUSTRAC%20Money%20Laundering%20NRA.pdf\n",
    "\n",
    "\n",
    "PREDICATE_CRIME_KEYWORDS = {\n",
    "    \"illicit drugs\": [\"drug trafficking\", \"methamphetamine\", \"heroin\", \"cocaine\", \"narcotics\"],\n",
    "    \"tax and revenue crime\": [\"tax evasion\", \"tax fraud\", \"revenue fraud\"],\n",
    "    \"government-funded program fraud\": [\"welfare fraud\", \"program fraud\", \"medicare fraud\"],\n",
    "    \"scams\": [\"scam\", \"phishing\", \"online fraud\", \"investment scam\"],\n",
    "    \"illicit tobacco\": [\"illicit tobacco\", \"illegal cigarettes\", \"tobacco smuggling\"],\n",
    "    \"pure cybercrime\": [\"cybercrime\", \"hacking\", \"ransomware\", \"malware\"],\n",
    "    \"identity crime\": [\"identity theft\", \"fake identity\", \"synthetic identity\"],\n",
    "    \"corruption and bribery\": [\"corruption\", \"bribery\", \"kickback\", \"embezzlement\"],\n",
    "    \"superannuation fraud\": [\"superannuation fraud\", \"pension fraud\", \"retirement fraud\"],\n",
    "    \"child sexual exploitation\": [\"child exploitation\", \"child abuse material\", \"CEM\"],\n",
    "    \"environmental crime\": [\"illegal logging\", \"wildlife trafficking\", \"environmental crime\"],\n",
    "    \"payment fraud\": [\"credit card fraud\", \"payment fraud\", \"unauthorised transaction\"],\n",
    "    \"firearms trafficking\": [\"arms trafficking\", \"illegal weapons\", \"gun smuggling\"],\n",
    "    \"human trafficking\": [\"human trafficking\", \"forced labour\", \"modern slavery\"],\n",
    "    \"intellectual property crime\": [\"counterfeit goods\", \"piracy\", \"intellectual property theft\"],\n",
    "}\n",
    "\n",
    "def classify_predicate_crimes(df: pd.DataFrame, text_cols=[\"title\", \"abstract\"]):\n",
    "    def extract_predicates(text):\n",
    "        matches = set()\n",
    "        for label, keywords in PREDICATE_CRIME_KEYWORDS.items():\n",
    "            for kw in keywords:\n",
    "                if kw.lower() in text.lower():\n",
    "                    matches.add(label)\n",
    "        return list(matches)\n",
    "\n",
    "    df[\"combined_text\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    df[\"predicate_crimes\"] = df[\"combined_text\"].apply(extract_predicates)\n",
    "    return df.drop(columns=[\"combined_text\"])\n",
    "\n",
    "raw_df = classify_sectors(raw_df)\n",
    "raw_df = tag_jurisdictions(raw_df)\n",
    "raw_df = classify_predicate_crimes(raw_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f418b30-01f6-4fd2-931a-3bd50ab9e50c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predicate_exploded = raw_df[[\"id\", \"predicate_crimes\"]].explode(\"predicate_crimes\").dropna()\n",
    "predicate_df = (\n",
    "    predicate_exploded\n",
    "    .groupby(\"predicate_crimes\")\n",
    "    .agg(publication_count=(\"id\", \"nunique\"))\n",
    "    .reset_index()\n",
    "    .sort_values(\"publication_count\", ascending=False)\n",
    ")\n",
    "predicate_df.to_csv(EXPORT_DIR / \"predicate_crimes.csv\", index=False)\n",
    "\n",
    "sector_exploded = raw_df[[\"id\", \"sectors\"]].explode(\"sectors\").dropna()\n",
    "sector_df = (\n",
    "    sector_exploded\n",
    "    .groupby(\"sectors\")\n",
    "    .agg(publication_count=(\"id\", \"nunique\"))\n",
    "    .reset_index()\n",
    "    .sort_values(\"publication_count\", ascending=False)\n",
    ")\n",
    "sector_df.to_csv(EXPORT_DIR / \"sectors.csv\", index=False)\n",
    "\n",
    "jurisdiction_exploded = raw_df[[\"id\", \"jurisdictions\"]].explode(\"jurisdictions\").dropna()\n",
    "jurisdiction_df = (\n",
    "    jurisdiction_exploded\n",
    "    .groupby(\"jurisdictions\")\n",
    "    .agg(publication_count=(\"id\", \"nunique\"))\n",
    "    .reset_index()\n",
    "    .sort_values(\"publication_count\", ascending=False)\n",
    ")\n",
    "jurisdiction_df.to_csv(EXPORT_DIR / \"jurisdictions.csv\", index=False)\n",
    "\n",
    "print(\"Extra export complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02ab9dc-3023-4bcc-ac69-6182b67eaf9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS fincrime;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4777db14-833e-4eee-ad62-bc7e5daf0b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# helper to write a pandas df as a Delta table\n",
    "def write_delta_from_pandas(pdf: pd.DataFrame, table_name: str, schema: str = \"fincrime\"):\n",
    "    sdf = spark.createDataFrame(pdf)\n",
    "    full_name = f\"{schema}.{table_name}\"\n",
    "    (\n",
    "        sdf.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(full_name)\n",
    "    )\n",
    "    print(f\"Written Delta table: {full_name}\")\n",
    "\n",
    "# core bibliometrics\n",
    "write_delta_from_pandas(pubs_per_year_df,       \"publications_per_year\")\n",
    "write_delta_from_pandas(pubs_by_country_df,     \"top_countries\")\n",
    "write_delta_from_pandas(pubs_by_institution_df, \"top_institutions\")\n",
    "write_delta_from_pandas(top_cited_df,           \"top_cited\")\n",
    "write_delta_from_pandas(works_flat_df,          \"works_tidy\")\n",
    "\n",
    "# AUSTRAC-style tags\n",
    "write_delta_from_pandas(predicate_df,    \"predicate_crimes\")\n",
    "write_delta_from_pandas(sector_df,       \"sectors\")\n",
    "write_delta_from_pandas(jurisdiction_df, \"jurisdictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708407c8-41d3-429d-8f25-34e21f7d684b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bridge tables (relational)\n",
    "predicate_exploded.rename(columns={\"predicate_crimes\": \"predicate_crime\"}, inplace=True)\n",
    "sector_exploded.rename(columns={\"sectors\": \"sector\"}, inplace=True)\n",
    "jurisdiction_exploded.rename(columns={\"jurisdictions\": \"jurisdiction\"}, inplace=True)\n",
    "\n",
    "write_delta_from_pandas(works_flat_df,      \"fact_works\")                \n",
    "write_delta_from_pandas(predicate_exploded, \"bridge_work_predicate\")\n",
    "write_delta_from_pandas(sector_exploded,    \"bridge_work_sector\")\n",
    "write_delta_from_pandas(jurisdiction_exploded, \"bridge_work_jurisdiction\")\n",
    "write_delta_from_pandas(inst_df,            \"bridge_work_institution\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7225206010921053,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_financial_crime_research_insights",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
